---
layout: post
title: variational autoencoder interactive demos with deeplearn.js
disqus_identifier : d8b05c89-d7aa-4f19-897b-7c52914a030c
feature: false
publish: true
category: AI
image:
    url: /assets/images/vae_graph.png
    alt: twins

---




<p>Hello Deaplearn.js VAE!</p>



<h2>Inro</h2>
<p>
    <span class="dropcap">I</span>f you are here, you probably know variational autoencoders are a different animal than vanilla autoencoders.
    It took me a while to get my head around them. The number of blog posts on the topic is itself evident that I wasn't alone in my struggle.
    In the end, what really helped me understand them, was this <a href="https://www.youtube.com/watch?v=7G4_Y5rsvi8" target="_blank">lecture by Ali Ghodsi</a> of University of Waterloo.
    He does an excellent job explaining the variational method and deriving all the mathematical formulas step by step.
    But he doesn't get to training and “reparameterization trick”.
</p>
<p>
    Since the network has random sampling at heart, it is at first not obvious how to do error backpropagation.
    “reparameterization trick” is a clever way of getting random sampling out of the error backpropagation path.
    To understand it, just compare the two graphs on the hero image on top of this page, after you've watched Ali Ghodsi's lecture.
</p>
<p>
    I guess you are here because I promised an interactive demo, so let's get to it.
</p>
<h2>Demos</h2>
<p>
    First a little bit about what you are seeing. <br>
    In my quest to understand VAEs, I coded one up from strach using PyTorch.
    (<a href="https://github.com/siarez/VAE" target="_blank">GitHub link here</a>)
    My script has a little function that saves model parameters (i.e. weights and biases) in a JSON file.
    I've used it to save the model parameters after training.
    Those parameters are now loaded into Deeplearn.js variables, and are used to create the interactive visualizations in this demo.
</p>
<p>
    The 10 sliders are the z (or \(\mu\)) that are fed to the decoder part of the network.
    As you move them, new images are generated by the decoder network.
</p>
<p>
    The 10 little tiles you see down left are sample reconstructed images. 
    When you click on them, the slider positions are set to the z values that generated them 
</p>
<p>
    The more fun part is drawing the input yourself. 
    When you use the drawing pad, your drawing is first encoded using the encoder network.
    The encoder network generates \(\mu\) and \(\sigma\). Each one is 10 dimensional.
    Here for simplicity, instead of sampling from the distributions, I throw out the \(\sigma\) and pass \(\mu\) to the decoder network.
</p>

<div class="row justify-content-between  mt-5">
    <div class="slidecontainer col-6 align-self-center"></div>
    <div id="mnist_reconstruction" class=" col-6">
    <i style="display:block">recostructed image</i>
    </div>
    </div>
    <div class="row justify-content-between mt-5">
    <div id="mnist_sample" class="col-6"></div>
    <div class="col-6">
        <button id="reset-btn">Clear drawing</button>
        <div class="pixel-picker-container">
            <div class="pixel-container"></div>
        </div>    
    </div>
</div>

<p>
    Try drawing some digits, and look at how the reconstructed image changes as you are drawing.
    You can try to draw things other that digits. 
    But you'll see it has difficaulty reconstructing them, or the reconstructed image will look like a digit.
</p>
<p>Have fun playing around, and share your observations :)</p>
<p>P.S. the hero image is taken from the
    <a href="https://arxiv.org/pdf/1606.05908.pdf" target="_blank">Tutorial on Variational Autoencoders</a>
    paper which I highly recommend.
</p>


<script src="https://unpkg.com/deeplearn"></script>
<script src="{{ site.baseurl }}/assets/js/pixel-picker.js" type="text/javascript"  defer='defer'></script>
<script src="{{ site.baseurl }}/assets/js/vae.js"  type="text/javascript" defer='defer'></script>

