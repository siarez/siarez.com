<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title></title>
    <description></description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Thu, 22 Jun 2017 12:10:45 -0700</pubDate>
    <lastBuildDate>Thu, 22 Jun 2017 12:10:45 -0700</lastBuildDate>
    <generator>Jekyll v3.4.3</generator>

      <item>
        <title>quora question pairs</title>
        <description>&lt;p&gt;
    &lt;span class=&quot;dropcap&quot;&gt;T&lt;/span&gt;hey have a problem at Quora.com. There are 13+M questions are already on their platform and ~13k questions added everyday.
    People hop on there and ask questions that have been asked and answered before.
    It is not humanly possible to read that many questions and flag the duplicates… unless you are &lt;a href=&quot;https://youtu.be/sinh-Mn-_RI?t=48s&quot; target=&quot;_blank&quot;&gt;her&lt;/a&gt;.
&lt;/p&gt;
&lt;p&gt;
    To address the issue they developed their &lt;a target=&quot;_blank&quot; href=&quot;https://engineering.quora.com/Semantic-Question-Matching-with-Deep-Learning&quot;&gt;own algorithms&lt;/a&gt; to detect duplicate question.
    On top of that, a while ago Quora published their first public dataset of question pairs publicly
    for machine learning (ML) engineers to see if anyone can come up with a better algorithm to detect duplicate questions,
    and they created a competition on Kaggle.
&lt;/p&gt;
&lt;p&gt;
    Here is how the competition works: ML engineers and ML engineer wannabes -cough-me-cough- who have too much time on their hand (or are not properly supervised at work),
    download the competition’s training set (which looks like fig. 1) and develop machine learning algorithms that learn by going through the examples in the training set.
    The training set  is usually manually labeled. In this case, 1 or 0 in the is_duplicate column indicates whether the questions are identical.
    Here the training set contains ~420K question pairs.
&lt;/p&gt;
&lt;figure class=&quot;figure&quot;&gt;
    &lt;table class=&quot;table&quot;&gt;
        &lt;thead&gt;
            &lt;tr&gt;
                &lt;th&gt;id&lt;/th&gt;
                &lt;th&gt;question1&lt;/th&gt;
                &lt;th&gt;question2&lt;/th&gt;
                &lt;th&gt;is_duplicate&lt;/th&gt;
            &lt;/tr&gt;
        &lt;/thead&gt;
        &lt;tbody&gt;
            &lt;tr&gt;
                &lt;th scope=&quot;row&quot;&gt;1&lt;/th&gt;
                &lt;td&gt;Why my answers are collapsed?&lt;/td&gt;
                &lt;td&gt;Why is my answer collapsed at once?&lt;/td&gt;
                &lt;td&gt;0&lt;/td&gt;
            &lt;/tr&gt;
            &lt;tr&gt;
                &lt;th scope=&quot;row&quot;&gt;2&lt;/th&gt;
                &lt;td&gt;How do I post a question in Quora?&lt;/td&gt;
                &lt;td&gt;How do I ask a question in Quora?&lt;/td&gt;
                &lt;td&gt;1&lt;/td&gt;
            &lt;/tr&gt;
            &lt;tr&gt;
                &lt;th scope=&quot;row&quot;&gt;3&lt;/th&gt;
                &lt;td&gt;Can I fit my booboos in a 65ml jar?&lt;/td&gt;
                &lt;td&gt;Is 1 baba worth 55 booboo (おっぱい)  ☃?&lt;/td&gt;
                &lt;td&gt;0&lt;/td&gt;
            &lt;/tr&gt;
        &lt;/tbody&gt;
    &lt;/table&gt;
    &lt;figcaption class=&quot;figure-caption&quot;&gt;Fig.1&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;
    Beside the training set, they also publish a test set which does not have the answers (is_duplicate column is empty).
    Your job is to develop an algorithm that learns to detect a duplicate question pair by looking at the training dataset.
    After it is done learning, you’ll have it go through each question pair in the test set and fill out the is_duplicate column.
    There are 2,345,796 question pairs in the test set.
&lt;/p&gt;
&lt;p&gt;
    Recently, I decided to pick up on an old passion of mine which is AI and ML.
    I also find Natural Language Processing/Understanding (NLP/U) very fascinating.
    This project was the sweet spot of all those things.
    Plus, I could use it as an excuse to sharpen my Python skills, and learn Tensorflow.
    I decided to use a couple of different models on this problem, all using Tensorflow.
&lt;/p&gt;
&lt;h2&gt;linear regression on cosine similarity&lt;/h2&gt;
&lt;p&gt;
    First to get my feet wet and establish a baseline, I coded up a very basic logistic regression model in Tensorflow.
    The model took only one feature as input, and that was cosine similarity of two sentences.
    Where each word in a question was represented by a vector from the pre-trained &lt;a href=&quot;https://nlp.stanford.edu/projects/glove/&quot; target=&quot;_blank&quot; &gt;GloVe&lt;/a&gt; word vector.
    This way each question becomes a matrix. Multiplying the matrices, summing up the elements,
    and normalizing by question lengths should give us a number (feature) that is proportional to how similar the questions are.
    I called this feature the &lt;i&gt;overlap_score&lt;/i&gt;.
    This should work for two reasons:
&lt;/p&gt;
&lt;ol&gt;
    &lt;li&gt;
        &lt;p&gt;If a word is present in both questions,
            then dot product of that word by itself gives highest possible result, 1 (vectors are normalized)
        &lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
        &lt;p&gt;Word vectors of synonym words most often have similar vectors.
            Which again means their dot product should be a higher number. &lt;/p&gt;
    &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;
    So this feature should not only capture same word appearing in the questions,
    but also a word and its synonym.
    This is great, right?! right?!
&lt;/p&gt;
&lt;p&gt;
    If you have been into machine learning, you know that one feature doesn’t get you very far.
    So needless to say, my single feature logistic regression had an accuracy that was slightly better than chance!
    facing the horrible performance, I decided to plot this feature against the ground truth to see if a relationship can be seen.
    fig. 2 shows how the overlap_score is related to the ground truth.
    As you can see there is no obvious relationship between the two.
&lt;/p&gt;
&lt;figure class=&quot;figure&quot;&gt;
    &lt;img class=&quot;img-fluid zoomable&quot; src=&quot;/assets/images/figure_1_with_normalization_whole_data.png&quot;&gt;
    &lt;figcaption class=&quot;figure-caption&quot;&gt;Fig.2&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;
    At this point I could try adding more features.
    That would have a definitely improved the result (as described by Abhishek &lt;a href=&quot;https://www.linkedin.com/pulse/duplicate-quora-question-abhishek-thakur&quot; target=&quot;_blank&quot; &gt;here&lt;/a&gt;),
    but I wanted to try something sexier than logistic regression.
&lt;/p&gt;
&lt;h2&gt;RNN fun&lt;/h2&gt;
&lt;p&gt;
    Since my renewed interest in machine learning, I came across a very interesting concept of recurrent neural networks or RNNs.
    Simply put, RNNs are a type of network that receives as part of its input, its output from the previous time step.
    So they are great for learning variable length sequences which is what sentences are.
    &lt;a href=&quot;http://karpathy.github.io/2015/05/21/rnn-effectiveness/&quot; target=&quot;_blank&quot;&gt;Here is a great post, if you want to learn more about RNNs&lt;/a&gt;.
&lt;/p&gt;
&lt;p&gt;
    So I started experimenting with a character level RNN implemented in Tensorflow.
    To get my feet wet with RNNs, I had a very simple strategy.
    During training I would concatenate question1, question2, and is_duplicate like this:
&lt;/p&gt;
&lt;code&gt;Why my answers are collapsed?Why is my answer collapsed at once?(NUL)0
&lt;/code&gt;
&lt;p&gt;I added a (NUL) ASCII character before is_duplicate to help the RNN learn that it should only output either a 0 or a 1 after the (NUL) and not any other character.
    My aim was to keep pumping these sequences in the RNN and hope it learns to predict is_duplicate correctly.
    (Yea, I'm that optimistic.)
&lt;/p&gt;
&lt;p&gt;
    This is a pretty bad strategy for many reasons.
    For one, because RNNs tend to put more weight on the last inputs in the sequence.
    So q1 would have less bearing on the prediction than q2, just because it is seen earlier and at the time of prediction is already distant memory.
    There are ways around this like bi-directional RNNs.
    But, my goal was more to play around with an RNN than to create a good model, so I stopped spending more time on this.
    I can report that the model did learn to predict ones and zeros, but I didn’t spend enough effort to measure its performance.
    From looking at the output, I was pretty sure it wasn’t going to be impressive.
    So I moved to my next shiny strategy.
&lt;/p&gt;
&lt;h2&gt;Decomposable Attention Model&lt;/h2&gt;
&lt;p&gt;
    One day during my morning swim in arxiv.org, I came across a curious paper titled:
    &lt;a href=&quot;https://arxiv.org/pdf/1606.01933v1.pdf&quot; target=&quot;_blank&quot;&gt;A Decomposable Attention Model for Natural Language Inference&lt;/a&gt;.
    The authors bragged about the state of the art performance of their model with an order of magnitude less variables than the next best thing.
    With about a week to go to the competition deadline, I decided to give this ago, and implemented their model in Tensorflow.
&lt;/p&gt;
&lt;p&gt;
    The model was originally tested on the Stanford Natural Language Inference (SNLI) dataset by the authors of the paper.
    I adapted this model for use in the Quora competition. Fig. 3 below depicts the model.
    Notice that the model is symmetric, and I didn’t draw the left side.
    (In my implementation, F, G, and H are each two layers deep. So this is not a very deep model overall. )
&lt;/p&gt;
&lt;figure class=&quot;figure&quot;&gt;
    &lt;img class=&quot;img-fluid zoomable&quot; src=&quot;/assets/images/quora-kaggle-model-diagram.png&quot;&gt;
    &lt;figcaption class=&quot;figure-caption&quot;&gt;Fig.3&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h3&gt;Training&lt;/h3&gt;
&lt;p&gt;
    I used vanilla SGD with constant learning rate.
    I spent very little time tuning the hyper parameters, just enough to make sure the loss is decreasing.
    Fig. 4 shows that the cross validation accuracy flats out and stays around 0.82 after 10 epochs.
&lt;/p&gt;
&lt;figure class=&quot;figure&quot;&gt;
    &lt;img class=&quot;img-fluid zoomable&quot; src=&quot;/assets/images/x-val_accuracy.png&quot;&gt;
    &lt;figcaption class=&quot;figure-caption&quot;&gt;Fig.4&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;
    Then I let the model run for 50 epochs. Fig. 5 shows the accuracy on the training set.
&lt;/p&gt;
&lt;figure class=&quot;figure&quot;&gt;
    &lt;img class=&quot;img-fluid zoomable&quot; src=&quot;/assets/images/training_loss_50epochs.png&quot;&gt;
    &lt;figcaption class=&quot;figure-caption&quot;&gt;Fig.5&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;
    Comparing this to fig. 4, you'll see even though the accuracy on x-val set flats out after about 4 epochs,
    The accuracy on the training set keeps going up.
&lt;/p&gt;
&lt;h3&gt;Performance - speed&lt;/h3&gt;
&lt;p&gt;
    As you can see from fig. 4 and fig. 5 the model is not very fast.
    That is attributed to the fact that my implementation doesn’t use batching. Here is why:
&lt;/p&gt;
&lt;p&gt;
    The sentences come in various lengths, then they are multiplied together, concatenated, and so on.
    This creates matrices of various sizes along the way which makes batching difficult.
    You are probably thinking &lt;i&gt;why not pad all sentence to a fix length?&lt;/i&gt;
    The average sentence length is ~12 words with standard deviation of 6 words, and the longest sentence is ~123 words.
    So padding all sentences to 123 words, would not yield any gains.
&lt;/p&gt;
&lt;p&gt;
    One way to overcome this issue is to bucket the sentences according to their lengths, and padding sentences in each bucket to max length of that bucket.
    For instance, sentences in with length 0 to 20 will be padded to 20, sentences with length between 20 to 40 will be padded to 40, and so on.
    This way groups of sentences with the same length can be batched together.
    However, the deadline was fast approaching, so I didn’t implement this bucketing and batching approach.
&lt;/p&gt;
&lt;p&gt;
    Without any batching, the gain from using a GPU were not that impressive.
    The model processes ~220 sentence pairs per second on my dedicated linux box with a GTX-1060 6Gb GPU where GPU utilization stands around 10%.
    The model runs at ~160 sentence pairs per second on my 2013 MacBook Pro with 2.4GHz i5 CPU.
    (As a side note, I saw a 12x speed improvement with my GPU on the Tensorflow’s PTB example!)
&lt;/p&gt;

&lt;h3&gt;Performance - accuracy&lt;/h3&gt;
&lt;p&gt;
    The model achieved ~95% accuracy on training data and 82% on x-val. set after 50 epochs (Which is similar to what Quora’s internal team had achieved with the same method).
    This model achieved a logloss of 0.448 on the competition test set, which landed me at ~2200 on the leaderboard.
    To put this in perspective, first team on the leaderboard had a logloss of 0.116.
&lt;/p&gt;
&lt;h3&gt;Future improvements&lt;/h3&gt;
&lt;p&gt;
    Time did not allow me to use TF-IDF, Stop Words, any kind of feature engineering, or ensembling and gradient boosting.
    I also did not implement the “infra-sentence” attention explained in the paper.
    Also for the sake of simplicity, I used a pretty crude pre-processing technique that gets entirely rid of some useful information that the model could leverage, like numbers and out-of-vocabulary words.
    I think all these areas of improvement are worth exploring and would result in noticeable improvement.
&lt;/p&gt;
&lt;h3&gt;Unforeseen challenges&lt;/h3&gt;
&lt;p&gt;
    One problem that I spent a ridiculous amount of time on was reading the CSV files and feeding them to Tensorflow.
    Both Python’s CSV reader and &lt;a href=&quot;https://www.tensorflow.org/programmers_guide/reading_data#csv_files&quot; target=&quot;_blank&quot;&gt;Tensorflow’s CSV decoder&lt;/a&gt; threw exceptions at various points in the file.
    The only hassle free CSV reader was the one from &lt;a href=&quot;https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html&quot; target=&quot;_blank&quot;&gt;Pandas&lt;/a&gt;.
&lt;/p&gt;
&lt;p&gt;
    Tensorflow documentation can also be a lot better.
    Frequently, I found myself looking at the code rather than the documentation to understand what it’s doing.
    This is something that I expect to get better as Tensorflow matures. Right now, things are moving too fast.
&lt;/p&gt;
&lt;h3&gt;Code&lt;/h3&gt;
&lt;p&gt;
    My Tensorflow implemetaion for the Decomposable Attention Model can be found here:
    &lt;a href=&quot;https://github.com/siarez/sentence_pair_classifier&quot; target=&quot;_blank&quot;&gt;siarez/sentence_pair_classifier&lt;/a&gt;
&lt;/p&gt;
&lt;h2&gt;further reading&lt;/h2&gt;
&lt;p&gt;
    If you are interested in learning more about what others have done for this competition make sure to check these links:
&lt;/p&gt;
&lt;ul&gt;
    &lt;li&gt;&lt;a href=&quot;https://engineering.quora.com/Semantic-Question-Matching-with-Deep-Learning&quot; target=&quot;_blank&quot;&gt;Semantic Question Matching with Deep Learning&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://explosion.ai/blog/quora-deep-text-pair-classification&quot; target=&quot;_blank&quot;&gt;Deep text-pair classification with Quora’s 2017 question dataset&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;http://www.erogol.com/duplicate-question-detection-deep-learning/&quot; target=&quot;_blank&quot;&gt;Duplicate Question Detection with Deep Learning on Quora Dataset&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://www.linkedin.com/pulse/duplicate-quora-question-abhishek-thakur&quot; target=&quot;_blank&quot;&gt;Is That a Duplicate Quora Question?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Fri, 16 Jun 2017 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/2017/06/16/quora-question-pairs.html</link>
        <guid isPermaLink="true">http://localhost:4000/2017/06/16/quora-question-pairs.html</guid>


      </item>

      <item>
        <title>Let the journey begin</title>
        <description>&lt;p&gt;
    &lt;span class=&quot;dropcap&quot;&gt;Y&lt;/span&gt;ou don’t have to dig too deeply to find out that high-end audio is a controversial and divisive field among consumers and enthusiasts.
    I find that many heated discussions are driven by the participants’ lack of knowledge. The knowledge is out there.
    There are many knowledgeable professionals working in the field, but they are removed from the public.
&lt;/p&gt;
&lt;p&gt;
    Marketing departments meticulously control the company’s message.
    Most audio companies are not interested in educating their customers.
    They would rather play it safe, so as not to offend anyone’s closely held beliefs—whatever helps close the deal.
    Furthermore, the way most audio journalists/reviewers work is also not helping this situation.
    Because many audio publications are supported by the same companies, they feel obliged to perpetuate their clients’ marketing messages.
&lt;/p&gt;
&lt;p&gt;
    At Serene Audio, we would rather challenge our customers and ourselves.
    That is the only way to learn, make progress, and move forward. Any company that claims to have found the ultimate truth in perfect sound reproduction and to have created the ultimate product is not sincere and is doomed to stay stagnant.
    We don’t claim to have done either of those, and we strive to gradually improve our knowledge and our products.
    In fact, the speakers that we make today are better than the speakers we made a year ago, and we will keep improving them.
&lt;/p&gt;
&lt;p&gt;
    We don’t hold any beliefs that we regard as the ultimate truth.
    The only belief that we hold dear is that advancement comes from evidence-based knowledge, so we are open to changing our mind when faced with new knowledge.
    If the new findings go against our currently held beliefs, then we will learn and adapt.
    This blog is where we share our knowledge and thoughts as we learn.
    If you are one of us, we would love for you to join us on this journey by following our blog and participating in the comments.
&lt;/p&gt;
</description>
        <pubDate>Fri, 29 May 2015 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/2015/05/29/let-the-journey-begin.html</link>
        <guid isPermaLink="true">http://localhost:4000/2015/05/29/let-the-journey-begin.html</guid>


      </item>

  </channel>
</rss>
